{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import argparse\n",
        "import scipy\n",
        "from scipy import ndimage\n",
        "import numpy as np\n",
        "import sys\n",
        "from packaging import version\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data, model_zoo\n",
        "from model.deeplab import Res_Deeplab\n",
        "from model.deeplab_multi import DeeplabMulti\n",
        "from model.deeplab_vgg import DeeplabVGG\n",
        "from dataset.gta5_dataset import GTA5DataSet\n",
        "from collections import OrderedDict\n",
        "import os\n",
        "from PIL import Image\n",
        "from utils.tool import fliplr\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "\n",
        "from config import CONSTS\n",
        "\n",
        "IMG_MEAN = np.array((104.00698793,116.66876762,122.67891434), dtype=np.float32)\n",
        "\n",
        "# We just use this file to evaluate the perfromance on the training set\n",
        "DATA_DIRECTORY = CONSTS.GTA_PATH\n",
        "DATA_LIST_PATH = CONSTS.GTA_TRAIN_LIST_PATH\n",
        "SAVE_PATH = CONSTS.GTA_RESULT_PATH\n",
        "\n",
        "IGNORE_LABEL = 255\n",
        "NUM_CLASSES = 19\n",
        "NUM_STEPS = 500 # Number of images in the validation set.\n",
        "RESTORE_FROM = 'http://vllab.ucmerced.edu/ytsai/CVPR18/GTA2Cityscapes_multi-ed35151c.pth'\n",
        "RESTORE_FROM_VGG = 'http://vllab.ucmerced.edu/ytsai/CVPR18/GTA2Cityscapes_vgg-ac4ac9f6.pth'\n",
        "RESTORE_FROM_ORC = 'http://vllab1.ucmerced.edu/~whung/adaptSeg/cityscapes_oracle-b7b9934.pth'\n",
        "SET = 'val'\n",
        "\n",
        "MODEL = 'Deeplab'\n",
        "\n",
        "palette = [128, 64, 128, 244, 35, 232, 70, 70, 70, 102, 102, 156, 190, 153, 153, 153, 153, 153, 250, 170, 30,\n",
        "           220, 220, 0, 107, 142, 35, 152, 251, 152, 70, 130, 180, 220, 20, 60, 255, 0, 0, 0, 0, 142, 0, 0, 70,\n",
        "           0, 60, 100, 0, 80, 100, 0, 0, 230, 119, 11, 32]\n",
        "zero_pad = 256 * 3 - len(palette)\n",
        "for i in range(zero_pad):\n",
        "    palette.append(0)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "def colorize_mask(mask):\n",
        "    # mask: numpy array of the mask\n",
        "    new_mask = Image.fromarray(mask.astype(np.uint8)).convert('P')\n",
        "    new_mask.putpalette(palette)\n",
        "    return new_mask\n",
        "\n",
        "def get_arguments():\n",
        "    args = easydict.EasyDict({\n",
        "        \"--model\": MODEL,\n",
        "        \"--data-dir\": DATA_DIRECTORY,\n",
        "        \"--data-list\": DATA_LIST_PATH,\n",
        "        \"--ignore-label\": IGNORE_LABEL,\n",
        "        \"--num-classes\": NUM_CLASSES,\n",
        "        \"--restore-from\": RESTORE_FROM,\n",
        "        \"--gpu\": 0,\n",
        "        \"--batchsize\": 10,\n",
        "        \"--set\": SET,\n",
        "        \"--save\": SAVE_PATH,\n",
        "    })\n",
        "    return args\n",
        "\n",
        "def main():\n",
        "    \"\"\"Create the model and start the evaluation process.\"\"\"\n",
        "\n",
        "    args = get_arguments()\n",
        "\n",
        "    gpu0 = args.gpu\n",
        "    batchsize = args.batchsize\n",
        "\n",
        "    model_name = os.path.basename( os.path.dirname(args.restore_from) )\n",
        "    args.save += model_name\n",
        "\n",
        "    if not os.path.exists(args.save):\n",
        "        os.makedirs(args.save)\n",
        "\n",
        "    if args.model == 'Deeplab':\n",
        "        model = DeeplabMulti(num_classes=args.num_classes, train_bn = False, norm_style = 'in')\n",
        "    elif args.model == 'Oracle':\n",
        "        model = Res_Deeplab(num_classes=args.num_classes)\n",
        "        if args.restore_from == RESTORE_FROM:\n",
        "            args.restore_from = RESTORE_FROM_ORC\n",
        "    elif args.model == 'DeeplabVGG':\n",
        "        model = DeeplabVGG(num_classes=args.num_classes)\n",
        "        if args.restore_from == RESTORE_FROM:\n",
        "            args.restore_from = RESTORE_FROM_VGG\n",
        "\n",
        "    if args.restore_from[:4] == 'http' :\n",
        "        saved_state_dict = model_zoo.load_url(args.restore_from)\n",
        "    else:\n",
        "        saved_state_dict = torch.load(args.restore_from)\n",
        "\n",
        "    try:\n",
        "        model.load_state_dict(saved_state_dict)\n",
        "    except:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "        model.load_state_dict(saved_state_dict)\n",
        "    model.eval()\n",
        "    model.cuda()\n",
        "\n",
        "    testloader = data.DataLoader(GTA5DataSet(args.data_dir, args.data_list, crop_size=(640, 1280), resize_size=(1280, 640), mean=IMG_MEAN, scale=False, mirror=False),\n",
        "                                    batch_size=batchsize, shuffle=False, pin_memory=True)\n",
        "\n",
        "\n",
        "    if version.parse(torch.__version__) >= version.parse('0.4.0'):\n",
        "        interp = nn.Upsample(size=(640, 1280 ), mode='bilinear', align_corners=True)\n",
        "    else:\n",
        "        interp = nn.Upsample(size=(640, 1280 ), mode='bilinear')\n",
        "\n",
        "    sm = torch.nn.Softmax(dim = 1)\n",
        "    for index, batch in enumerate(testloader):\n",
        "        if (index*batchsize) % 100 == 0:\n",
        "            print('%d processd' % (index*batchsize))\n",
        "        image, _, _, name = batch\n",
        "        print(image.shape)\n",
        "\n",
        "        inputs = Variable(image).cuda()\n",
        "        if args.model == 'Deeplab':\n",
        "            output1, output2 = model(inputs)\n",
        "            output_batch = interp(sm(0.5* output1 + output2)).cpu().data.numpy()\n",
        "            #output1, output2 = model(fliplr(inputs))\n",
        "            #output2 = fliplr(output2)\n",
        "            #output_batch += interp(output2).cpu().data.numpy()\n",
        "        elif args.model == 'DeeplabVGG' or args.model == 'Oracle':\n",
        "            output_batch = model(Variable(image).cuda())\n",
        "            output_batch = interp(output_batch).cpu().data.numpy()\n",
        "\n",
        "        output_batch = output_batch.transpose(0,2,3,1)\n",
        "        output_batch = np.asarray(np.argmax(output_batch, axis=3), dtype=np.uint8)\n",
        "\n",
        "        for i in range(output_batch.shape[0]):\n",
        "            output = output_batch[i,:,:]\n",
        "            output_col = colorize_mask(output)\n",
        "            output = Image.fromarray(output)\n",
        "\n",
        "            name_tmp = name[i].split('/')[-1]\n",
        "            output.save('%s/%s' % (args.save, name_tmp))\n",
        "            output_col.save('%s/%s_color.png' % (args.save, name_tmp.split('.')[0]))\n",
        "\n",
        "    return args.save"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with torch.no_grad():\n",
        "    save_path = main()\n",
        "os.system('python compute_iou.py ./data/GTA5/data/gtFine/val %s'%save_path)"
      ],
      "outputs": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}